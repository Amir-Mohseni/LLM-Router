{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SSyDSBMQ52Le"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fzo0ui656Md",
        "outputId": "92903b8e-c288-46a1-fa07-2cf8c7ba78be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-12 17:08:33 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "import json\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XeBJQ4FU6Bst"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "llm_names = [\"google/gemma-3-1b-it\", \"google/gemma-3-4b-it\"]\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
        "dataset = ds['problem'][:5]\n",
        "k_responses = 5  # Number of responses per question\n",
        "temperature = 0.7  # Sampling temperature for diversity\n",
        "max_tokens = 512  # Maximum tokens per response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ppCtBWBC6O47"
      },
      "outputs": [],
      "source": [
        "# Initialize the model and output file dynamically\n",
        "def initialize_model_and_output_file(llm_id):\n",
        "    llm_name = llm_names[llm_id]\n",
        "    output_file = f\"{llm_name.replace('/', '_')}_responses.json\"\n",
        "    return llm_name, output_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jm06ELSj7kXs"
      },
      "outputs": [],
      "source": [
        "# Generate responses using vLLM\n",
        "def generate_responses(llm_name, prompts, k_responses):\n",
        "    # Initialize the LLM model\n",
        "    llm = LLM(model=llm_name, max_model_len=2048, dtype=\"auto\")\n",
        "\n",
        "    # Set sampling parameters for diverse responses\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        top_p=0.95,\n",
        "        max_tokens=max_tokens,\n",
        "        n=k_responses  # Specify the number of responses to generate per prompt\n",
        "    )\n",
        "\n",
        "    # Generate responses for each prompt\n",
        "    all_responses = []\n",
        "    for prompt in prompts:\n",
        "        predictions = llm.generate([prompt], sampling_params)\n",
        "        responses = [prediction.outputs[0].text for prediction in predictions]  # Access generated text directly\n",
        "        all_responses.append({\"prompt\": prompt, \"responses\": responses})\n",
        "\n",
        "    return all_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "c8yrUS1w7mpR"
      },
      "outputs": [],
      "source": [
        "# Save responses to a JSON file\n",
        "def save_responses_to_file(responses, output_file):\n",
        "    with open(output_file, \"w\") as f:\n",
        "        json.dump(responses, f, indent=4)\n",
        "    print(f\"Responses saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "e0b9a3d61de24ad7913c288dfc91cbd0",
            "70e9e882af844ddf94eba93795403f8e",
            "3a2e10723e014865a0d927fb592bb3e3",
            "cb466245ade449498207163f7fceba88",
            "1716e56325f64f039ece0cd01a44485e",
            "48789d1972304054af2ccdd1f5e061b6",
            "00838b6b6743483fa8f8f5933d246a93",
            "97c8f26f1d1e424c85dbbfeef9884027",
            "b607c5049411462285a40b5013b0cba6",
            "a0761f8952044b848a8ff1fddb0c752b",
            "0ef48310004d4772be79a3bf967a49b6"
          ]
        },
        "id": "ZeN7FgXB7okL",
        "outputId": "7ab4ca3b-0e37-4f32-b627-95ae7a1a3f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating responses using google/gemma-3-1b-it...\n",
            "INFO 04-12 17:08:53 [config.py:600] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
            "INFO 04-12 17:08:53 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
            "INFO 04-12 17:08:58 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='google/gemma-3-1b-it', speculative_config=None, tokenizer='google/gemma-3-1b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-3-1b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 04-12 17:08:58 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7b5f4454e410>\n",
            "INFO 04-12 17:08:59 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 04-12 17:08:59 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "INFO 04-12 17:08:59 [gpu_model_runner.py:1258] Starting to load model google/gemma-3-1b-it...\n",
            "WARNING 04-12 17:09:00 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 04-12 17:09:00 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "INFO 04-12 17:09:01 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0b9a3d61de24ad7913c288dfc91cbd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-12 17:09:01 [loader.py:447] Loading weights took 0.75 seconds\n",
            "INFO 04-12 17:09:02 [gpu_model_runner.py:1273] Model loading took 1.9148 GiB and 2.198860 seconds\n",
            "INFO 04-12 17:09:14 [backends.py:416] Using cache directory: /root/.cache/vllm/torch_compile_cache/b77175cda9/rank_0_0 for vLLM's torch.compile\n",
            "INFO 04-12 17:09:14 [backends.py:426] Dynamo bytecode transform time: 12.39 s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank0]:W0412 17:09:17.527000 6556 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-12 17:09:21 [backends.py:132] Cache the graph of shape None for later use\n",
            "INFO 04-12 17:10:00 [backends.py:144] Compiling a graph for general shape takes 44.68 s\n",
            "INFO 04-12 17:10:26 [monitor.py:33] torch.compile takes 57.07 s in total\n",
            "INFO 04-12 17:10:28 [kv_cache_utils.py:578] GPU KV cache size: 331,440 tokens\n",
            "INFO 04-12 17:10:28 [kv_cache_utils.py:581] Maximum concurrency for 2,048 tokens per request: 161.84x\n",
            "INFO 04-12 17:11:06 [gpu_model_runner.py:1608] Graph capturing finished in 39 secs, took 0.43 GiB\n",
            "INFO 04-12 17:11:07 [core.py:162] init engine (profile, create kv cache, warmup model) took 124.66 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 5/5 [00:06<00:00,  1.34s/it, est. speed input: 3.73 toks/s, output: 382.30 toks/s]\n",
            "Processed prompts: 100%|██████████| 5/5 [00:06<00:00,  1.25s/it, est. speed input: 3.99 toks/s, output: 408.70 toks/s]\n",
            "Processed prompts: 100%|██████████| 5/5 [00:06<00:00,  1.25s/it, est. speed input: 3.99 toks/s, output: 408.24 toks/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Responses saved to google_gemma-3-1b-it_responses.json\n"
          ]
        }
      ],
      "source": [
        "llm_id = 0\n",
        "\n",
        "llm_name, output_file = initialize_model_and_output_file(llm_id)\n",
        "\n",
        "print(f\"Generating responses using {llm_name}...\")\n",
        "\n",
        "# Generate responses for the dataset prompts\n",
        "responses = generate_responses(llm_name, dataset, k_responses)\n",
        "\n",
        "# Save the generated responses to a file\n",
        "save_responses_to_file(responses, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u688ik1y7zFg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00838b6b6743483fa8f8f5933d246a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ef48310004d4772be79a3bf967a49b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1716e56325f64f039ece0cd01a44485e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a2e10723e014865a0d927fb592bb3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c8f26f1d1e424c85dbbfeef9884027",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b607c5049411462285a40b5013b0cba6",
            "value": 1
          }
        },
        "48789d1972304054af2ccdd1f5e061b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e9e882af844ddf94eba93795403f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48789d1972304054af2ccdd1f5e061b6",
            "placeholder": "​",
            "style": "IPY_MODEL_00838b6b6743483fa8f8f5933d246a93",
            "value": ""
          }
        },
        "97c8f26f1d1e424c85dbbfeef9884027": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0761f8952044b848a8ff1fddb0c752b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b607c5049411462285a40b5013b0cba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb466245ade449498207163f7fceba88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0761f8952044b848a8ff1fddb0c752b",
            "placeholder": "​",
            "style": "IPY_MODEL_0ef48310004d4772be79a3bf967a49b6",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00&lt;00:00,  1.69it/s]\n"
          }
        },
        "e0b9a3d61de24ad7913c288dfc91cbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70e9e882af844ddf94eba93795403f8e",
              "IPY_MODEL_3a2e10723e014865a0d927fb592bb3e3",
              "IPY_MODEL_cb466245ade449498207163f7fceba88"
            ],
            "layout": "IPY_MODEL_1716e56325f64f039ece0cd01a44485e"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
