version: '3.10'

services:
  llm-router:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      # Mount these directories for persistent data storage
      - ./data_collection/inference_results:/app/data_collection/inference_results
      - ./extracted_answers:/app/extracted_answers
      # Mount .env for configuration
      - ./.env:/app/.env
    environment:
      # Set environment variables here (will override .env)
      - PYTHONPATH=/app
      # Optional: Add API keys here or use .env file
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
      # - VLLM_API_KEY=${VLLM_API_KEY}
    # Run in interactive mode with pseudo-TTY
    tty: true
    stdin_open: true
    # Default command provides a shell
    command: bash
    # Add network to allow communication with vllm-server
    networks:
      - llm-network

  # Add vLLM server as a separate service
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
    command: serve_vllm
    ports:
      - "8000:8000"
    # GPU configuration for tensor parallelism
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]
    volumes:
      - ./.env:/app/.env
      # Cache models to avoid re-downloading
      - ./models:/app/models
    environment:
      - PYTHONPATH=/app
      - HF_HOME=/app/models
      # Number of GPUs to use for tensor parallelism can be set here or via command
      # - VLLM_TENSOR_PARALLEL_SIZE=2
    # Custom command to specify tensor parallelism - uncommment and adjust as needed
    # command: serve_vllm --tensor-parallel-size 2
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge 